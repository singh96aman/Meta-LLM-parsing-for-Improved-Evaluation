% \section{Background} \label{sec:background}

% \textcolor{red}{TODO}

\section{Methodology} 
\label{sec:methodology}

% \kc{Describe metrics and how to perform  benchmarks here so that in the next section we talk about what specific experiments we are running? Right now they are kind of mixed I think.}

We evaluate the strengths and weaknesses of the LLM-as-a-judge paradigm by focusing on the relatively straight-forward assessment task of judging answers to the questions of a knowledge-reasoning benchmark.
We consider \njudges \judgemodels, that assess the responses of \nexamtakers \evaluatormodels. 
%
The benchmark questions are provided to the \evaluatormodels and their responses are recorded to be evaluated by the \judgemodels.
%

% \section{Experiments}

% Results of a benchmark, as evaluated by the judge, can be used in different ways â€“ establishing a
% relative ordering of LLMs based on their performance [cite], creating a quantitative measure of their
% capabilities in a particular domain [cite], developing/distilling other LLMs, etc. The metrics used to77
% gauge the quality of the evaluations from a judge, therefore, depend on the final use-cases for the
% benchmark results and the reliability of the benchmark results.
% To understand the strengths and weaknesses of different judges, we benchmark pre-trained and80
% instruction-tuned evaluation models across a wide variety of model sizes, and examine the quality
% of the evaluations from different judge models. Specifically, we conduct experiments to answer the
% following research questions: 1) How well do the evaluations from different judges align with human
% evaluations? 2) How is a alignment of evaluations related to the alignment of the final scores given
% by the judges? 3) How similar are the rankings of evaluation models generated by the judge models
% compared to the rankings by humans? 4) How sensitive are the judge models to the specific prompt
% provided to them to give a judgement? 5) What are the similarities and differences in the mistakes
% made by different judges?

\subsection{Benchmark and evaluation metrics} 
\label{sec:experiments:benchmark}
We use the TriviaQA  dataset\citep{joshi2017triviaqa} consisting of 95K question-answer pairs sourced from 14 trivia and quiz league websites. 
%
Each question in the train and validation set is annotated with alist of short answers containing a minimal set of facts and evidence documents collected from Wikipedia and the Web.
The benchmark is done on the validation set and short answers are used as references for the judges while grading. 
The training and validation sets both come from the \texttt{unfiltered} partition of the dataset.
%
To stay closer to the typical scenarios in which LLMs may be used as judges, we focus on questions with ten or fewer reference answers.

For the experiments that require manual annotation of the \evaluatormodel responses, the benchmarks are done on a random sample of $400$ questions from the dataset.
%
For other experiments that do not require human annotation, the entire validation set is used for the benchmarks.
%
We primarily use score, alignment, and Cohen's kappa coefficient to evaluate judges. The definitions of each of the metrics is explained in detail in \cref{app:metrics}
%  

\subsection{\Evaluatormodels} \label{sec:experiments:evaluationmodel}
To understand the strengths and weaknesses of different judges, we benchmark pre-trained (base) and instruction-tuned (chat) \evaluatormodels across a wide variety of model sizes, and examine the quality of the evaluations from different \judgemodels.
In particular, we consider \eval{Llama-2} in 7B, 13B, and 70B parameter sizes for both base and chat versions \citep{touvron2023llama}, \eval{Mistral 7B} in base and chat versions \citep{jiang2023mistral}, and \eval{GPT-4 Turbo} \citep{achiam2023gpt} as the \evaluatormodels. 

The prompts for the \evaluatormodels contain five few-shot examples of (question, answer) pairs from the TriviaQA training set.
%
The prompts for the instruction-tuned models additionally include a command signaling the model to answer the given question in a succinct manner similar to the provided examples.
%
See \cref{app:prompt-templates} for details and examples.
% In our case study, we consider the core knowledge benchmark TriviaQAchatmodels.

% Based on our initial findings, which indicate that pre-trained models outperform fine-tuned models on knowledge reasoning benchmarks, we benchmark various pre-trained models and their instruction-tuned counterparts to further investigate this observed behavior.

% For the benchmark evaluation of the base models, we add a question from the validation set to the prompt. The model is then tasked with answering this additional unanswered question in the prompt. The prompt can be found in Appendix A.1.1

% To improve the chat models for question answering benchmarks, we include an additional command in the prompt as seen in Appendix A.1.1. This command informs the chat model that it is participating in a question-answering benchmark and should answer questions concisely. This extra instruction helps ensure that the model responds briefly and directly, avoiding verbose answers. 

% The model's response is truncated at the '\textbackslash nQ:' character and saved in the database. We truncate the answer at this point because the models, after answering the question in the prompt,  begin to generate their own questions and answer them. The stored response is then evaluated against its references using various evaluation methods.
 

\subsection{\Judgemodels} \label{sec:experiments:judgellm}
To get a comprehensive view of the strengths and weaknesses of \judgemodels across different model sizes and architectures, we use instruction-tuned versions of \judge{Llama-2} in 7B, 13B, and 70B sizes \citep{touvron2023llama}, \judge{Llama-3} in 8B and 70B sizes \citep{meta2024llama3}, \judge{Mistral} 7B \citep{jiang2023mistral}, \judge{GPT-4 Turbo} \citep{openai2024gpt4}, \judge{Gemma} 2B \citep{gemma2024gemma}, and \judge{JudgeLM} 7B \citep{judgelm} as judges (see  \cref{app:judgeLLM_details} for further details).
%
The judges are instructed to respond with only a single word,  \texttt{``correct''} or \texttt{``incorrect''}. % , based on the references. 
The prompts can be found in \cref{app:judge-prompt-template}. The names of all \evaluatormodels and \judgemodels are shown in \cref{tab:evaluation}.
For ease of reading the \judge{\judgemodels} are depicted in a different font than the \eval{\evaluatormodels}.


% \subsection{Evaluator \& Judge Models}
% In this case study, we utilize Llama-2 [cite] in 7B, 13B, and 70B parameter sizes for both base and chat versions, Mistral-7B [cite] in base and chat versions and GPT-4 Turbo [cite] as evaluators \kc{Base models as evaluators?}. 
% Based on our initial findings \kc{Need to show these results}, which indicated that pre-trained models outperformed fine-tuned models on CKBs, we benchmark various pretrained-aligned model pairs and further investigate this observed behavior.

% % Additionally, by employing models of different sizes, we aim to obtain diverse responses. This approach allows for a wider range of responses, providing the Judge LLM with a more comprehensive spectrum for assessment. 

% The models Llama-2 chat in 7B, 13B, and 70B sizes, Llama-3 Instruct in 8B and 70B sizes, Mistral 7B Instruct-v0.2, GPT 4 Turbo, Gemma 2B FT and BAAI JudgeLM-7B-v1.0, are used as Judges.

% These models of varying architectures and parameter sizes will allow us to examine their alignment with human ground truth and assess how parameter sizes impact the LLM's judgment. We aim to observe the relationship between model sizes and the stringency of evaluation. We also aim to examine the models' inherent biases in evaluation and identify the models best suited for this type of objective assessment.

% \subsection{How we obtain benchmark answers for each model}
%  In this case study, we employ a 5-shot setting to assess the model on the benchmark. We randomly select five questions from the training dataset along with their answers, and use these question-answer pairs to construct a prompt. 

% For the benchmark evaluation of the base models, we add a question from the validation set to the prompt. The model is then tasked with answering this additional unanswered question in the prompt. The prompt can be found in Appendix A.1.1

% To improve the chat models for question answering benchmarks, we include an additional command in the prompt as seen in Appendix A.1.1. This command informs the chat model that it is participating in a question-answering benchmark and should answer questions concisely. This extra instruction helps ensure that the model responds briefly and directly, avoiding verbose answers. 

% The model's response is truncated at the '\textbackslash nQ:' character and saved in the database. We truncate the answer at this point because the models, after answering the question in the prompt,  begin to generate their own questions and answer them. The stored response is then evaluated against its references using various evaluation methods.
 
% \subsection{How we use LLMs as a judge}
% The Judge LLM is given the question, its corresponding references and the evaluator model's response and is instructed to evaluate the answer in a single word; either as "correct" or "incorrect", based on the references. The prompt can be found in Appendix A.1.2 (d) 

% We refrain from providing specific instructions on how to evaluate answers in the prompt to avoid introducing human bias into the Judge LLM's evaluation criteria and its fundamental definition of correctness. Instead, we provide the Judge LLM with human annotation guidelines in the prompt during judging to assess how they affect the Judge LLM's scores and alignment with humans. These guidelines can be found in Section 5.1.2.

% \subsection{Evaluation Strategies ?}

% % We also assess the LLM's performance as a judge using other evaluation metrics, 
% We also assess the LLM's performance as a judge using other classical lexical evaluation techniques like exact match and contains match. In exact match evaluation, if the model's answer exactly matches any of the reference answers for a question, it is evaluated as correct. This evaluation is performed without considering letter casing.

% Similarly, contains match evaluation considers an answer correct if the response contains all the words found in any of the reference answers, regardless of their order. Like exact match evaluation, contains match evaluation is also performed without considering letter casing.

% The evaluation scores from the Judges represent the percentage of questions that the judges deemed correctly answered by the evaluator models, out of the total number of questions in the sample size.

% During human annotation, questions marked as correct by the exact match evaluation are skipped, as they are assumed to be annotated correctly by humans. However, these questions are still counted in the final score.


\begin{table}[htbp]
    \centering
    \renewcommand{\arraystretch}{1.3} % Increase row height
    \begin{tabular}{|>{\centering\arraybackslash}m{4cm}|>{\centering\arraybackslash}m{9cm}|}
        \hline
        \textbf{\Evaluatormodels  \hspace{0.5cm} (base \& instruction-tuned)} & \eval{Llama-2 (7B, 13B and 70 B)}, \eval{Mistral 7B}, \eval{GPT-4 Turbo} \\
        \hline
        \multirow{2}{4cm}{\centering \textbf{\Judgemodels (instruction-tuned)}} & \judge{Llama-2 (7B, 13B, and 70B)}, \judge{Llama-3 (8B and 70B)}, \\
         & \judge{Gemma 2B}, \judge{Mistral 7B}, \judge{JudgeLM 7B}, \judge{GPT-4 Turbo} \\
        \hline
    \end{tabular}
    \captionsetup{skip=8pt} % Adjust space between table and caption
    \caption{The \evaluatormodels and \judgemodels we use in our experiments. We consider a wide variety of \judgemodels; to get a comprehensive overview of their (potential) biases, we consider \evaluatormodels of various sizes and types.}
    \label{tab:evaluation}
\end{table}


% \vspace{-1.5em}
% \subsection{Metrics} \label{sec:experiments:metrics}

% Evaluation Score\\
% Cohen's Kappa
% Percentage Alignment
% Precision, Recall, Ranking



\subsection{Baselines and human evaluators \label{sec:experiments:baselineandhumanannotation}}

We compare the assessments of the \judgemodels with classical lexical evaluation techniques to provide a baseline and use human annotations to get gold-standard answers. Details on the baselines and human annotations are as follows:

\paragraph{Baselines}
As baselines, we use two commonly used lexical evaluation techniques  -- exact match (EM) and contains match.
For EM, a response is considered correct if the response exactly matches one of the reference answers for the given question.
For contains match evaluation, an answer is considered correct if at least one of the reference answers is a sub-string of the response string.
Both EM and contains match are computed in a case-insensitive manner.
% Like exact match evaluation, contains match evaluation is also performed without considering letter casing.
% We compare the judge LLMs assessments with various classical lexical evaluation techniques, such as exact match and contains match, as well as human judgements. In exact match evaluation, if the model's answer exactly matches any of the reference answers for a question, it is evaluated as correct. This evaluation is performed without considering letter casing.
%
% Similarly, contains match evaluation considers an answer correct if the response contains all the words found in any of the reference answers, regardless of their order. Like exact match evaluation, contains match evaluation is also performed without considering letter casing.
%
% The evaluation scores from the judges represent the percentage of questions that the judges deemed correctly answered by the evaluator models, out of the total number of questions in the sample.

\paragraph{Human judgements}
As a ground-truth assessment, we obtain human annotations for each \evaluatormodel answer.
To compute human inter-annotator agreement as well as fine-tune human evaluation guidelines, we first conduct an experiment in which three humans judge 600 \eval{Llama-2 7B} \citep{touvron2023llama} answers, randomly sampled from the triviaQA \citep{joshi2017triviaqa} dataset.
% The human annotators were asked to annotate \evaluatormodel responses that didn't exactly match with one of the references. 
We then determine collective ``Human Judgement'' is determined through a majority vote.
%
% During human annotation, questions marked as correct by the exact match evaluation are skipped, as they are assumed to be annotated correctly by humans. However, these questions are still counted in the final score.
%
In this experiment, the average alignment among human evaluators with the Human Judgement had a Cohen's kappa score \citep{cohen1960kappa} of \textbf{96.36\%$\pm$1.67\%} (human guidelines can be found in \cref{app:human_annotation_guidelines}).The average percentage alignment was \textbf{98.33\%$\pm$0.76\%}
Given this near-perfect alignment score, in the rest of our experiments we consider only one human evaluator, to reduce the cost of human annotations throughout.
We human evaluate (the same) 400 questions for each \evaluatormodel.
%
% After converging on evaluation criteria, we manually annotate 400 questions for each evaluation model.
% Given the high cost of human annotations and near-perfect human alignment between annotators, each evaluation model is annotated by only one human annotator for 400 random sample. 
