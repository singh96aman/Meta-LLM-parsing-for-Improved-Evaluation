\section {Introduction} \label{sec:intro}


Over the last few years, Large Language models (LLMs) have been shown to possess remarkable world knowledge \citep{petroni2019language, razeghi2022impact} and the ability to learn specialized tasks from a few examples \citep{brown2020language}. 
LLMs have been employed in various tasks including generating free-form responses, condensing extensive textual data, conducting searches, categorizing or grouping documents, and facilitating question-answering systems \citep{vectara_llm_use_cases}. 
As more and more new LLMs with different architectures and training methods continue to be released, there is a need for tools that can accurately evaluate their capabilities and limitations across a variety of tasks. 

However, the empirical evaluation of LLMs has so far shown to be a challenging and non-trivial task, primarily due to the diversity of output they generate and the wide range of tasks they are used for \citep{Zhang2024LLMEval, li2023generative}. 
Various methods have been proposed for evaluating LLMs, typically falling into one of two broad categories. 
Benchmarks such as MMLU \citep{mmlu}, TruthfulQA \citep{lin2021truthfulqa}, and GSM8K \citep{cobbe2021training} are used to evaluate specific capabilities of LLMs in an automated manner. 
Additionally, leaderboards like Chatbot Arena \citep{chiang2024chatbot} and Open LLM Leaderboard \citep{open-llm-leaderboard} assign ranks to models considering pair-wise rankings of LLM outputs, done by humans or, in some cases, automated evaluation methods.

Since both strategies involve evaluating free-form text responses generated by the LLMs, evaluating the responses is often just as challenging as generating them in the first place \citep[see e.g.][]{chang2023survey}. 

%One solution to this problem is to formulate benchmarks as multiple-choice questions (MCQ), and compare the log-probabilities of the potential answers rather than evaluating the generated answer directly. 
%However, the MCQ paradigm severely limits the range of abilities that can be evaluated, and this setup increasingly diverges from how LLMs are used in practice.

The use of lexical matching methods such as exact match (EM) or n-gram overlap to evaluate the responses are practical and cost-efficient approaches, but are susceptible to false negatives and often fail to adequately distinguish between responses with subtle differences that change their semantic meaning.
%
This issue is exacerbated when evaluating instruction-tuned ``chat'' models that are fine-tuned to carry out conversations with humans in natural language, since their responses tend to be more verbose \citep{saito2023verbosity, renze2024benefits}. 

\kc{Should this paragraph go in Related Work?}
For these reasons, human evaluation remains the gold standard for evaluating LLM responses. %, especially since many benchmarks aim to assess how useful the LLMs are to humans. 
However, human evaluation is expensive, time-consuming, and often impractical in many use cases. 
As a result, it has increasingly become common practice to evaluate LLM responses using another LLM as a \judgemodel \citep[e.g.][]{lin2021truthfulqa,islam2023financebench,chiang2023can,liusie2024llm}.
Prior work has demonstrated that LLMs such as \judge{GPT-4} exhibit high alignment with humans in such tasks when used as a judge \citep{sottana2023evaluation,zheng2024judging}. 
Despite promising results in various settings, \judgemodels still suffer from the issues of current LLMs, such as hallucinations and factual errors \citep{ye2023cognitive, turpin2023language} and difficulty in following complex instructions \citep{li2023instruction, he2024can}. Moreover, use of LLMs as judges can give rise to unique challenges such as exhibiting position bias \citep{pezeshkpour2023large} and verbosity bias \citep{saito2023verbosity} in their preference, confusing the evaluation criteria \citep{hu2024llm}, or focusing more on the style and grammar of the response compared to its factuality \citep{wu2023style}.
% However, the strengths, weaknesses and biases of this \emph{LLM-as-a-judge} paradigm have not been studied in detail.
%
% As much of the contemporary research in LLMs and vision-language models is guided by empirical results, it is important to ensure that the results of various benchmarks for LLMs reflect the true capabilities of the models and are not artifacts of the various choices surrounding the benchmarks and their evaluations.

In this work, we assess the accuracy of LLMs as judges and investigate their properties, comparing them with human evaluation and automated evaluation methods. 
\textbf{The primary contribution of this work is an extensive study of how well various popular LLMs perform when acting as judges, along with an analysis of how their performance varies across different \judgemodels and evaluation strategies}. 
While previous studies on the accuracy of LLMs as judges have relied on measuring the alignment of LLM judgments with human judgments, we find that LLMs with similar human alignment can differ significantly on the questions they evaluate incorrectly. 
\textbf{The second contribution of this work is an investigation of the ``characters and styles'' of different LLMs as judges, and their susceptibility to different kinds of errors}.

% Key results: There is variation in scores given by judges, even ones with high alignment. There is pre-trained to instruction-tuned unlearning happening, but not to the degree suggested by EM.
