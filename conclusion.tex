\section {Conclusion}

In summary, our research reveals significant misalignment between Judge LLM evaluations and human assessments, with no consistent patterns in final scores or rankings even when evaluations align. But, our study also demonstrates that lexical matching strategies, such as "contains match," outperform half of the Judge LLMs while preserving the accuracy of exam taker rankings as the best Judge model. Notably, GPT-4 emerges as the most consistent LLM across all metrics, albeit at a higher cost. However, a more cost-effective alternative could be utilizing LLama 3 70B as a Judge. Furthermore, we observe that smaller models tend to be more lenient, whereas larger models consistently yield superior results as Judge LLMs. 
We also find that Judge LLMs are highly sensitive to prompt variations, with performance declining with overly complex or ambiguous instructions. Furthermore, our comparison highlights the consistent underperformance of fine-tuned models compared to pre-trained ones in objective assessments. These findings underscore the need for refinement in automatic grading system and use of Judge LLMs.



% In conclusion, we report that evaluations from different judges align poorly with human judgement. Judge LLMs do not exhibit any signs of systematic biases that may help us make an argument to use them. We also 

% \kc{Basically summarize here what are the answers that we found for the research questions that we put forward in the Experiments section. All answers should directly follow form the results of the experiments we performed.}


% textbf{1)} How well do the evaluations from different judges align with human evaluations? \textbf{2)} How is a alignment of evaluations related to the alignment of the final scores given by the judges? \textbf{3)} How similar are the rankings of evaluation models generated by the judge models compared to the rankings by humans? \textbf{4)} How sensitive are the judge models to the specific prompt provided to them to give a judgement? \textbf{5)} What are the similarities and differences in the mistakes made by different judges?

% \subsection{Future Work}
% These baseline results are a lot different from the previous run I did, with the difference being a slight change in the template (delimiting the questions, answers and references with “```” ), and skipping high ref count questions (although I don’t think the second change would make a difference). So that shows that the exact template used can be a big factor in the evaluations, and even GPT-4 is susceptible to small changes in the template.